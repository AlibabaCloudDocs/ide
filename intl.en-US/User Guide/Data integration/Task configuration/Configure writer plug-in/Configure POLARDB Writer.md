# Configure POLARDB Writer {#concept_l5l_p1c_5fb .concept}

This topic describes the data types and parameters supported by POLARDB Writer and how to configure it in both wizard and script modes.

POLARDB Writer writes data into a POLARDB database. At the underlying implementation level, POLARDB Writer connects to a remote POLARDB database through JDBC, and runs the `INSERT INTO...` or `REPLACE INTO...` SQL statement to write data into the database. Internally, data is submitted to the database in batches, and therefore the database must use the InnoDB engine.

**Note:** You must configure a data source before configuring POLARDB Writer. For more information, see [Configure a POLARDB data source](intl.en-US/User Guide/Data integration/Data source configuration/Configure a POLARDB data source.md#).

POLARDB Writer is designed for ETL developers to import data from data warehouses to POLARDB. POLARDB Writer can also be used as a data migration tool by DBA and other users. POLARDB Writer obtains protocol data generated by a reader through the Data Integration framework. The generated protocol data varies with the writeMode attribute that you have configured.

**Note:** The task shall at least have the `INSERT INTO... or REPLACE INTO...` permission. Whether other permissions are required depends on the SQL statements specified in the preSql and postSql attributes when you configure the task.

## Type conversion list {#section_ny3_4fr_5fb .section}

Similar to POLARDB Reader, POLARDB Writer supports most data types in POLARDB. Check whether a data type is supported before configuring POLARDB Writer.

POLARDB Writer converts the data types in POLARDB as follows:

|Type classification|POLARDB data type|
|:------------------|:----------------|
|Integer|Int, Tinyint, Smallint, Mediumint, Bigint, and Year|
|Float|Float, Double, and Decimal|
|String|Varchar, Char, Tinytext, Text, Mediumtext, and Longtext|
|Date and time|Date, Datetime, Timestamp, and Time|
|Boolean|Boolean|
|Binary|Tinyblob, Mediumblob, Blob, Longblob, and Varbinary|

## Parameter description {#section_vpb_wfr_5fb .section}

|Attribute|Description|Required|Default value|
|:--------|:----------|:-------|:------------|
|datasource|The data source name. It must be identical to the data source name added. Adding data sources is supported in script mode.|Yes|None|
|table|The name of the destination table.|Yes|None|
|writeMode|The write mode, which can be set to insert or replace.-   REPLACE INTO...: If no primary key conflict or unique index conflict occurs, the action is the same as that of INSERT INTO. If a conflict occurs, the fields in new rows replace all fields in original rows.
-   INSERT INTO...: If a primary key conflict or unique index conflict occurs, data cannot be written into the conflicting rows and is regarded as dirty data.
-   INSERT INTO table \(a,b,c\) VALUES \(1,2,3\) ON DUPLICATE KEY UPDATE...: If no primary key conflict or unique index conflict occurs, the action is the same as that of INSERT INTO. If a conflict occurs, the fields in new rows replace the specified fields in original rows.

|No|insert|
|column|The fields of the destination table into which data needs to be written. These fields are separated with commas. For example, `"column": ["id","name","age"]`. If you want to write all columns in turn, use the asterisk \(\*\), for example, `"column": ["*"]`.|Yes|None|
|preSql|The SQL statement to be run before the data synchronization task is run. For example, you can clear old data before data synchronization. Currently, you can run only one SQL statement in wizard mode, and multiple SQL statements in script mode.|No|None|
|postSql|The SQL statement to be run after the data synchronization task is run. For example, you can add a timestamp after data synchronization. Currently, you can run only one SQL statement in wizard mode, and multiple SQL statements in script mode.|No|None|
|batchSize|The number of records submitted at a time. This attribute can greatly reduce the frequency of interaction between Data Integration and POLARDB on the network, and increase the overall throughput. However, an excessively large value may lead to OOM during the data synchronization process.|No|1024|

## Development in wizard mode {#section_jvk_sgr_5fb .section}

1.  **Specify data sources**

    Configure the source and destination of data for a synchronization task.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/62209/155176864132014_en-US.png)

    |Parameter|Description|
    |:--------|:----------|
    |**Data Source**|The datasource attribute in the preceding parameter description. Select the data source that you have configured.|
    |**Table**|The table attribute in the preceding parameter description. Select the destination table.|
    |**Statements Run Before Import**|The preSql attribute in the preceding parameter description. Enter the SQL statement that is run before the data synchronization task is run.|
    |**Statements Run After Import**|The postSql attribute in the preceding parameter description. Enter the SQL statement that is run after the data synchronization task is run.|
    |**Primary Key Violation**|The writeMode attribute in the preceding parameter description. Select the expected write mode.|

2.  Configure mappings of fields \(the column attribute in the preceding parameter description\).

    Each source table field on the left maps a destination table field on the right. You can click **Add** to add a mapping or move the cursor over a line and click **Delete** to delete the current mapping.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/62209/155176864132015_en-US.png)

    |Configuration|Description|
    |:------------|:----------|
    |**Map Fields with the Same Name**|Click **Map Fields with the Same Name** to establish a mapping between fields with the same name. Note that the data type must be consistent.|
    |**Map Fields in the Same Line**|Click **Map Fields in the Same Line** to establish a mapping for the same row. Note that the data type must be consistent.|
    |**Remove Mappings**|Click **Remove Mappings** to remove mappings that have been established.|
    |**Auto Layout**|The fields are automatically sorted based on specified rules.|

3.  **Configure channel control**

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/62209/155176864132018_en-US.png)

    |Parameter|Description|
    |:--------|:----------|
    |**DMU**|The unit that measures the resources \(including CPU, memory, and network resources\) consumed by Data Integration. A DMU represents the minimum operating capability of a Data Integration task, that is, the data synchronization processing capability given limited CPU, memory, and network resources.|
    |**Concurrent Jobs**|The maximum number of threads used to concurrently read data from the source or write data into the data storage media in a data synchronization task. In wizard mode, you can configure the concurrency for a task on the wizard page.|
    |**Dirty Data Records Allowed**|The maximum number of errors or dirty data records allowed.|
    |**Task Resource Group**|The machines on which tasks are run. If a large number of tasks are run on the default resource group, some tasks may be delayed due to insufficient resources. In this case, we recommend that you add a custom resource group. Currently, a custom resource group can be added only in China \(Hangzhou\) and China \(Shanghai\). For more information, see [Add task resources](intl.en-US/User Guide/Data integration/Common configuration/Add task resources.md#).|


## Development in script mode {#section_vw5_p3r_5fb .section}

The following code is an example of configuration in script mode. For more information about attributes, see the preceding parameter description.

```
{
    "type": "job",
    "steps": [
        {
            "parameter": {},
            "name": "Reader",
            "category": "reader"
        },
        {
            "parameter": {
                "postSql": [],// The SQL statement to be run after the data synchronization task is run.
                "datasource": "test_005",// The data source name.
                "column": [// The destination table columns.
                    "id",
                    "name",
                    "age",
                    "sex",
                    "salary",
                    "interest"
                ],
                "writeMode": "insert",// The write mode.
                "batchSize": 256,// The number of records submitted at a time.
                "encoding": "UTF-8",// The encoding format.
                "table": "POLARDB_person_copy",// The destination table name.
                "preSql": []// The SQL statement to be run before the data synchronization task is run.
            },
            "name": "Writer",
            "category": "writer"
        }
    ],
    "version": "2.0",// The version number.
    "order": {
        "hops": [
            {
                "from": "Reader",
                "to": "Writer"
            }
        ]
    },
    "setting": {
        "errorLimit": {// The maximum number of errors allowed.
            "record": ""
        },
        "speed": {
            "concurrent": 6,// The number of concurrent threads.
            "throttle":false,// Indicates whether to throttle the transmission rate.
            "dmu": 6// The DMU value.
        }
    }
}
```

