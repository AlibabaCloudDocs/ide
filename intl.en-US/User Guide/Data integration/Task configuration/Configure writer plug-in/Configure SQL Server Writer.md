# Configure SQL Server Writer {#concept_sdt_fb4_q2b .concept}

This topic describes the data types and parameters supported by SQL Server Writer and how to configure Writer in both Wizard and Script mode.

The SQL Server Writer plug-in can be used to write data in target tables of the primary SQL Server database. At the underlying implementation level, the SQL Server Writer connects to a remote SQL Server database through Java Database Connectivity \(JDBC\), and runs the `insert into...` to write data in an SQL Server instance. The data is submitted to the database in batch within the instance.

The SQL Server Writer is designed for Extract, transform, load \(ETL\) developers to import data from data warehouses to the SQL Server. The SQL Server Writer can also be used as a data migration tool by DBA and other users.

The SQL Server Writer obtains protocol data \(`insert into...`\) generated by Reader through the Data Integration framework. If the primary key conflicts with the unique index, the data cannot be written in conflicting lines. To improve performance, use `PreparedStatement + Batch` and configure `rewriteBatchedStatements=true` to buffer data to the thread context buffer. Write requests are initiated only when the data volume in the buffer reaches the threshold.

**Note:** 

-   Data can be written into a target table only when the target table resides in the primary database.
-   The task must have the insert into... permission. Other permission requirements depend on statements specified in PreSQL and PostSQL when you configure the task.

## Type conversion list {#section_vcg_fvn_q2b .section}

The SQL Server Writer supports most data types in the SQL Server. Check whether the data type is supported before using the SQL Server Writer.

The SQL Server writer converts the list of types for SQL Server, as follows:

|Type classification|SQL server data types|
|:------------------|:--------------------|
|Integer| Bigint, Int, Smallint, and Tinyint

 |
|Float point|Float, Decimal, Real Numeric|
|String type| Char, Nchar, Ntext, Nvarchar, Text, Varchar, Nvarchar \(MAX\), and Varchar \(MAX\)

 |
|Date and time type|Date, Time, and Datetime|
|Boolean|Bit|
|Binary|Binary, Varbinary, Varbinary \(max\), and Timestamp|

## Parameter description​ {#section_jn2_gqh_p2b .section}

|Attribute|Description|Required|Default value|
|:--------|:----------|:-------|:------------|
|datasource|The data source name. The data source must be identical to the added data source. Adding data source is supported in Script Mode.|Yes|None|
|table|The name of the selected table that must be synchronized.|Yes|None|
|column|The required fields of the target table into which data is written. These fields are separated by commas \(,\). For example, `"column":["id","name","age"]`. If you want to write all columns in turn, use the asterisk \(\*\) representation. For example: `"column ": ["*"]`.|Yes|None |
|preSql|The SQL statement runs before running the data synchronization task. Currently, you can run only one SQL statement in Wizard Mode, and multiple SQL statements in Script Mode. For example: clear old data.|No|None |
|postSql|The SQL statement that runs after running the data synchronization task. Currently, you can run only one SQL statement in Wizard Mode, and more than one SQL statement in Script Mode. For example: add a timestamp.|No|None |
|writeMode|The specified import mode that allows data insertion.insert: If the primary key conflicts with the unique index, the Data Integration determines the data as dirty data, but retains the original data.

|No|Insert|
|batchSize|The number of records submitted in batch at a time can greatly reduce interactions between Data Integration and SQL Server over the network, and increase the overall throughput. However, an excessively large value may cause the running process of Data Integration to become Out of memory \(OOM\).|No|1,024|

## Development in Wizard Mode {#section_bp2_wsh_p2b .section}

1.  Choose source

    The following is the configuration item descriptions:

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/16255/15514335948218_en-US.png)

    Parameters:

    -   Data source: The datasource in the preceding parameter description section. Enter the configured data source name.
    -   Table: The table in the preceding parameter description section. Select the table for synchronization.
    -   Before import: The preSQL in the preceding parameter description section, namely, the SQL statement that runs before the data synchronization task run.
    -   After import: The postSQL in the preceding parameter description section, which is the SQL statement that runs after running the data synchronization task.
    -   Primary key conflict: The writeMode in the preceding parameter description section. You can select the expected import mode.
2.  The field mapping is the column in the above parameter description.

    The source table field on the left and the target table field on the right are in one-to-one relationships, click Add row to **Add A Single** field. To delete the current field, click **Delete**.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/16255/15514335958219_en-US.png)

    -   In-row mapping: You can click **Enable Same-Line Mapping** to create a mapping for the same row. Note that the data type must be consistent.
    -   Automatic formatting: The fields are automatically sorted based on corresponding rules.
3.  Control the tunnel

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/16221/15514335957675_en-US.png)

    Parameters:

    -   DMU: A unit that measures the resources consumed during data integration, including CPU, memory, and network bandwidth. One DMU represents the minimum amount of resources used for a data synchronization task.
    -   Concurrent job count: The maximum number of threads used to concurrently read/write data into the data storage media in a data synchronization task. You can configure a concurrency for the specified task in Wizard mode.
    -   The maximum number of errors indicates the maximum number of dirty data records.
    -   Task resource group: The machine on which the task runs. If the number of tasks is large, the default Resource Group is used to wait for a resource. We recommend that you add a Custom Resource Group \(currently only East China 1 and East China 2 supports adding custom resource groups\). For more information, see [Add scheduling resources](reseller.en-US/User Guide/Data integration/Common configuration/Add task resources.md#).

## Development in Script Mode {#section_cp2_wsh_p2b .section}

For more information on how to configure jobs written to SQL Server, see specific parameter completion in the preceding parameter description section .

```
{
    "type": "job",
    "version": 2.0 ", // version number
    "steps":{//The following is a reader template. You can find the corresponding reader plug-in documentations.
        {
            "stepType":"stream",
            "parameter":{},
            "name":"Reader",
            "category":"reader"
        },
        {
            "stepType": "sqlserver", // plug-in name
            "parameter": {
                "postSql": [], // SQL statement that was first executed after the data synchronization task was executed
                "datasource": "", // Data Source
                "column": [// Field
                    "id",
                    "name"
                ],
                "table":”", // table name
                "preSql": [] // SQL statement that was first executed before the data synchronization task was executed
            },
            "name":"Writer",
            "category":"writer"
        }
    ],
    "setting":{
        "errorLimit": {
            "record": "0"//Number of error records
        },
        "speed": {
            "throttle":false,//False indicates that the traffic is not throttled and the following throttling speed is invalid. True indicates that the traffic is throttled.
            "concurrent": "1",//Number of concurrent tasks
            "dmu": 1 // DMU Value
        }
    },
    "order":{
        "hops":[
            {
                "from":"Reader",
                "to":"Writer"
            }
        ]
    }
}
```

