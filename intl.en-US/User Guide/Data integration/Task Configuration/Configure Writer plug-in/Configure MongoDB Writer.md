# Configure MongoDB Writer {#concept_gmn_qwm_q2b .concept}

In this article we will show you the data types and parameters supported by MongoDB Writer and how to configure Writer in script mode.

The MongoDB Writer plug-in uses MongoClient, the Java client of MongoDB, to write data into MongoDB. The latest version of Mongo has reduced the granularity of DB locks from the DB level to the document level, with the powerful indexing capabilities of MongoDB, data sources are basically able to meet the requirements of writing data to MongoDB. The requirements for data updates can also be implemented by configuring the business primary key.

**Note:** 

-   Before you start configuring the MongoDB writer plug-in, first configure the data source. For more information, see[Configure MongoDB data source](reseller.en-US/User Guide/Data integration/Data source configuration/Configure MongoDB data source.md#).
-   If you are using ApsaraDB for MongoDB, a root account is provided by default.
-   To ensure security, Data Integration only supports using the relevant account of MongoDB for connection. Avoid using the root account as an access account when adding and using the MongoDB data source.

MongoDB Writer acquires the protocol data generated by Reader by means of the Data Integration framework, and converts the data types supported by Data Integration to the ones supported by MongoDB individually. The data integration itself does not support array types, but MongoDB supports array types, and the index of the array type is strong.

To use the MongoDB array type, you must convert the string to the array in MongoDB by using special configurations of parameters before writing data into MongoDB.

## Type conversion list {#section_m2v_hxm_q2b .section}

MongoDB Writer supports most data types in MongoDB. Check whether your data type is supported before using it.

MongoDB Writer converts the MongoDB data types as follows:

|Type Classification|MongoDB Data|
|:------------------|:-----------|
|Integer|INT and Long|
|Float|Double|
|String type|String and array|
|Date and time|Date|
|boolean|bool|
|Binary|Bytes|

## Parameter descriptionâ€‹ {#section_jn2_gqh_p2b .section}

|Attribute|Description|Required|Default Value|
|:--------|:----------|:-------|:------------|
|datasource|Data source name. It must be identical to the data source name added. Adding data source is supported in script mode.|Yes|None|
|Collectionname|The collection name of monogodb.|Yes|None|
|column|Description: An array of multiple column names of a document in MongoDB.-   name: The column name.
-   type: The Column type.
-   splitter: Special delimiter. It is used only when a string to be processed is split into character arrays by delimiters. Strings are split using the delimiter specified by this parameter and stored into MongoDB arrays.

|Yes|None|
|Writemode|Description: It specifies whether to overwrite data during transmission.-   isReplace: If this parameter is set to true, the data of the same replaceKey is overwritten. If it is set to false, the data is not overwritten.
-   replaceKey: It specifies the business primary key for each record entry and is used to overwrite data \(ReplaceKey must be unique and is generally the primary key in Mongo\).

|No|None|

## Development in wizard mode {#section_bp2_wsh_p2b .section}

Development in wizard mode is unavailable currently.

## Development in script mode {#section_cp2_wsh_p2b .section}

To configure data synchronization jobs written to MongoDB, please refer to the above parameter descriptions for details.

```
{
    "type": "job",
    "version": 2.0 ", // version number
    "steps":[
        {//The following is a reader template. You can find the corresponding reader plug-in documentations.
            "stepType":"stream",
            "parameter":{},
            "name": "Reader ",
            "category":"reader"
        },
        {
            "stepType":"hdfs",//plug-in name
            "parameter": {
                "path": "", //path
                "fileName": "ww",//File name
                "compress": "", // File compression type
                "datasource": "", // Data Source
                "column":[
                    {
                        "name": "col1", // field name
                        "type": "string" // Field Type
                    },
                    {
                        "name": "col2 ",
                        "type": "int"
                    },
                    {
                        "name":"col3",
                        "type":"double"
                    },
                    {
                        "name":"col4",
                        "type":"boolean"
                    },
                    {
                        "name":"col5",
                        "type": "date"
                    }
                ],
                "writeMode": "insert",//Write mode
                "fieldDelimiter": "," //Delimiter of each column
                "encoding": "UTF-8", // encoding format
                "fileType": "// text type
            },
            "name":"Writer",
            "category":"writer"
        }
    ],
    "setting":{
        "errorLimit": {
            "record": "0"//Number of error records
        },
        "speed": {
            "throttle":false,//False indicates that the traffic is not throttled and the following throttling speed is invalid. True indicates that the traffic is throttled.
            "concurrent": 1, // Number of job concurrency
            "dmu": 1 // DMU Value
        }
    },
    "order":{
        "hops":[
            {
                "from":"Reader",
                "to":"Writer"
            }
        ]
    }
}
```

