# Configure DRDS Writer {#concept_tgk_fkl_q2b .concept}

In this article we will show you the data types and parameters supported by DRDS Writer and how to configure Writer in both wizard mode and script mode.

The DRDS Writer plug-in provides the ability to write data to DRDS tables. At the underlying implementation level, DRDS Writer connects to the proxy of a remote DRDS database through JDBC, and writes data into DRDS by running the corresponding SQL statement `replace into....` The SQL statement in writes the data to the DRDS.

**Note:** Note that the SQL statement you run is replace into, and your table must have a primary key or a unique index to avoid data duplication. You must configure the data source before configuring the DRDS Writer plug-in. For more information, see [Configure DRDS data sources](reseller.en-US/User Guide/Data integration/Data source configuration/Configure DRDS data sources.md#)Configure the DRDS data source.

DRDS Writer is designed for ETL developers to import data from data warehouses to DRDS. DRDS Writer can also be used as a data migration tool by DBA and other users.

DRDS Writer acquires the protocol data generated by Reader by means of the CDP framework, and writes data into DRDS by running the statement `replace into....`  If the primary key does not conflict with the unique index, the system performs the same action with insert into. When a conflict exists, all the fields in the original line are replaced with the fields in the new line. DRDS Writer commits the accumulated data to DRDS's proxy, which then determines whether the data is written into one table or multiple tables, and how to route the data when it is written into multiple tables.

**Note:** The entire task should at least have the permission replace into....  Whether other permissions are required depends on the statements you specified in PreSQL and PostSQL when you configure the task.

Similar to MySQL Writer, DRDS Writer currently supports most data types in MySQL. Check whether your data type is supported.

DRDS Writer converts DRDS data types as follows:

|Type Classification|DRDS data type|
|:------------------|:-------------|
|Integer|int, tinyint, smallint, mediumint, int, bigint, and year|
|Floating point|float, double, and decimal|
|String|varchar, char, tinytext, text, mediumtext, and longtext|
|Date and time|date, datetime, timestamp, and time|
|Boolean|bit, and bool|
|Binary|tinyblob, mediumblob, blob, longblob, and varbinary|

## Parameter description​ {#section_jn2_gqh_p2b .section}

|Attribute|Description|Required|Default Value|
|:--------|:----------|:-------|:------------|
|datasource|Data source name. It must be identical to the data source name added. Adding data source is supported in script mode.| Yes|None|
|table|Description: The table selected for synchronization.| Yes|None|
|writeMode|Description: Select an import mode. The replace mode and insert ignore mode are supported.-   replace: If the primary key does not conflict with the unique index, the system performs the same operation with insert into.  When a conflict exists, all the fields in the original line are replaced with the fields in the new line.
-   insert ignore: If the primary key conflicts with the unique index, Data Integration ignores and discards the updated data with no logs.

| No|Insert ignore|
|column|Description: The fields of the target table into which data is required to be written. These fields are separated by commas. For example: "column": \["id", "name", "age"\]. Use \* if it is required to write data into all columns in sequence. For example: "column": \["\*"\].| Yes|None |
|preSql|Description: The SQL statement that is run before the data synchronization task is run. Currently, you can run only one SQL statement in wizard mode, and more than one SQL statement in script mode.For example, clear old data.| No|None |
|postSql|Description: The SQL statement that is run after the data synchronization task is run. Currently, you can run only one SQL statement in wizard mode, and more than one SQL statement in script mode. For example, add a timestamp.| No|None |
|batchSize|Description: The quantity of records submitted in one operation. This parameter can greatly reduce the interactions between Data Integration and MySQL over the network, and increase the overall throughput. However, the running process of Data Integration may become out of memory \(OOM\) if the value is too large.| No|1,024|

## Development in wizard mode {#section_bp2_wsh_p2b .section}

1.  Data source: 

    Configuration item descriptions:

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/16242/15411401958013_en-US.png)

    Parameters:

    -   Data source: The datasource in the preceding parameter description. Enter the data source name you configured.
    -   Table: table in the preceding parameter description. Select the table to be synchronized.
    -   Prepared statement before import: preSql in the preceding parameter description, namely, the SQL statement that is run before the data synchronization task is run.
    -   Post-import completion statement: postSql in the preceding parameter description, which is the SQL statement that is run after the data synchronization task is run.
2.  Field mapping: The column in the preceding parameter description.

    The Source Table Field on the left maps with the Target Table Field on the right. Click **Add Line**, and then a field is added. Hover the cursor over a line, click **Delete**, and then the line is deleted.

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/16242/15411401958014_en-US.png)

    -   In-row mapping: You can click **Enable Same-Line Mapping** to create a mapping for the same row. Note that the data type must be consistent.
    -       -   Automatic formatting: The fields are automatically sorted based on corresponding rules.
3.  Channel control

    ![](http://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/16221/15411401957675_en-US.png)

    Parameters:

    -   DMU: A unit which measures the resources \(including CPU, memory, and network bandwidth\) consumed during data integration. One DMU represents the minimum amount of resources used for a data synchronization task.
    -   Concurrent count: Maximum number of threads used to concurrently read data from or write data into the data storage media in a data synchronization task. In wizard mode, configure a concurrency for the specified task on the wizard page.
    -   Number of error records: The maximum number of dirty data records.
    -   Task Resource Group: the machine on which the task runs, if the number of tasks is large, the default Resource Group is used to wait for a resource, it is recommended that you add a Custom Resource Group \(currently only 1 East China, east China 2 supports adding custom resource groups\). For more information, see[Add scheduling resources](reseller.en-US/User Guide/Data integration/Common configuration/Add scheduling resources.md#).

## Development in script mode {#section_cp2_wsh_p2b .section}

Configure a job to write data into DRDS:

```
{
    "type":"job",
    "version":2.0", //version number
    "steps":[
        {//The following is a reader template. You can find the corresponding reader plug-in documentations.
            "stepType":"stream",
            "parameter":{}
            "name":"Reader",
            "category":"reader"
        },
        {
            "stepType":"drds",//plug-in name
            "parameter":{
                "postSql": [], // SQL statement executed after the data synchronization task is executed
                "datasource":"", // Data Source
                "column":[// column name
                    "id",
                ],
                "writeMode":"insert ignore ",
                "batchSize":"1024", //number of records submitted in one batch size
                "table":"test",//table name
                "postSql":[], //SQL statement executed after the data synchronization task is executed
            },
            "name":"Writer",
            "category":"writer"
        }
    ],
    "setting":{
        "errorLimit":{
            "record": "0"//Number of error records
        },
        "speed": {
            "throttle":false,//False indicates that the traffic is not throttled and the following throttling speed is invalid. True indicates that the traffic is throttled.
            "concurrent": "1", //Number of concurrency
            "dmu": 1 // Number of DMU
        }
    },
    "order":{
        "hops":[
            {
                "from":"Reader",
                "to": "Writer"
            }
        ]
    }
}
```

